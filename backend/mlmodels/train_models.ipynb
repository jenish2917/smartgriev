{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee0522f",
   "metadata": {},
   "source": [
    "# SmartGriev Model Training\n",
    "\n",
    "This notebook trains all three core models for the SmartGriev system:\n",
    "1. Sentiment Analysis Model\n",
    "2. Text Classification Model\n",
    "3. Named Entity Recognition (NER) Model\n",
    "\n",
    "Each model is optimized for grievance-related text in multiple Indian languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33275731",
   "metadata": {},
   "source": [
    "# Data Requirements\n",
    "\n",
    "## 1. Sentiment Analysis Data\n",
    "Required format in JSON:\n",
    "```json\n",
    "{\n",
    "    \"text\": \"The road condition is very poor and no action has been taken.\",\n",
    "    \"sentiment\": \"NEGATIVE\",\n",
    "    \"language\": \"en\"  // or \"hi\", \"ta\", etc.\n",
    "}\n",
    "```\n",
    "\n",
    "Sources:\n",
    "- [SAIL Dataset](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) - Customer Reviews\n",
    "- [PG Portal Data](https://pgportal.gov.in/) - Public Grievances\n",
    "- Municipal Corporation Databases\n",
    "\n",
    "## 2. Complaint Classification Data\n",
    "Required format:\n",
    "```json\n",
    "{\n",
    "    \"text\": \"Street light in sector 7 has not been working for 2 weeks\",\n",
    "    \"category\": \"UTILITIES\",\n",
    "    \"sub_category\": \"STREET_LIGHT\"\n",
    "}\n",
    "```\n",
    "\n",
    "Sources:\n",
    "- Municipal Corporation Complaint Systems\n",
    "- Smart City Grievance Portals\n",
    "- State Government Portals\n",
    "\n",
    "## 3. NER Training Data\n",
    "Required format:\n",
    "```json\n",
    "{\n",
    "    \"text\": \"Municipal Commissioner Mr. Kumar visited Gandhi Nagar on 15th January\",\n",
    "    \"entities\": [\n",
    "        [19, 28, \"PERSON\"],\n",
    "        [36, 47, \"LOC\"],\n",
    "        [51, 62, \"DATE\"]\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Sources:\n",
    "- Government Office Orders\n",
    "- Public Notices\n",
    "- Official Communications\n",
    "- Municipal Corporation Records\n",
    "\n",
    "## Data Collection Tools\n",
    "1. Web Scraping:\n",
    "   - Government websites\n",
    "   - Public grievance portals\n",
    "   - News articles about civic issues\n",
    "\n",
    "2. Official APIs:\n",
    "   - Smart City APIs\n",
    "   - Government Open Data Portals\n",
    "   - RTI Disclosure Portals\n",
    "\n",
    "3. Manual Annotation:\n",
    "   - Use tools like Label Studio\n",
    "   - BRAT Annotation Tool\n",
    "   - Prodigy for NER annotation\n",
    "\n",
    "## Sample Dataset Size Requirements:\n",
    "- Sentiment Analysis: Minimum 5000 labeled examples\n",
    "- Classification: 1000+ examples per category\n",
    "- NER: 2000+ sentences with annotated entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d60842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForTokenClassification\n",
    ")\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Constants\n",
    "MODELS_DIR = './saved_models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Model configurations\n",
    "CONFIG = {\n",
    "    'sentiment': {\n",
    "        'model_name': 'microsoft/mdeberta-v3-base',  # Multilingual DeBERTa\n",
    "        'num_labels': 3,\n",
    "        'labels': ['NEGATIVE', 'NEUTRAL', 'POSITIVE'],\n",
    "        'max_length': 128,\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'classification': {\n",
    "        'model_name': 'microsoft/mdeberta-v3-base',\n",
    "        'num_labels': 8,\n",
    "        'labels': ['CIVIC', 'LAW_AND_ORDER', 'UTILITIES', 'TRANSPORTATION', \n",
    "                  'EDUCATION', 'HEALTH', 'AGRICULTURE', 'OTHER'],\n",
    "        'max_length': 128,\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'ner': {\n",
    "        'labels': ['PERSON', 'ORG', 'LOC', 'DATE', 'FACILITY', 'DEPT'],\n",
    "        'embedding_dim': 200,\n",
    "        'lstm_units': 128,\n",
    "        'max_length': 100,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "def load_complaint_data():\n",
    "    \"\"\"Load complaint data from fixtures\"\"\"\n",
    "    with open('./fixtures/initial_data.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    complaints = []\n",
    "    for item in data:\n",
    "        if item['model'] == 'complaints.complaint':\n",
    "            complaints.append({\n",
    "                'text': item['fields']['description'],\n",
    "                'category': item['fields']['category'],\n",
    "                'sentiment': item['fields'].get('sentiment', 'NEUTRAL'),\n",
    "                'entities': item['fields'].get('entities', [])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(complaints)\n",
    "\n",
    "# Load and split data\n",
    "df = load_complaint_data()\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building functions\n",
    "def build_transformer_model(model_type):\n",
    "    \"\"\"Build a transformer-based model for sentiment or classification\"\"\"\n",
    "    config = CONFIG[model_type]\n",
    "    \n",
    "    # Create base model\n",
    "    base_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=config['num_labels']\n",
    "    )\n",
    "    \n",
    "    # Create input layers\n",
    "    input_ids = layers.Input(shape=(config['max_length'],), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = layers.Input(shape=(config['max_length'],), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Get outputs from base model\n",
    "    outputs = base_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Add custom layers for fine-tuning\n",
    "    x = layers.Dropout(0.1)(logits)\n",
    "    outputs = layers.Dense(config['num_labels'], activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(\n",
    "        inputs={'input_ids': input_ids, 'attention_mask': attention_mask},\n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_ner_model():\n",
    "    \"\"\"Build a custom BiLSTM-CRF model for NER\"\"\"\n",
    "    config = CONFIG['ner']\n",
    "    \n",
    "    # Input layer\n",
    "    input_text = layers.Input(shape=(config['max_length'],))\n",
    "    \n",
    "    # Embedding layer\n",
    "    x = layers.Embedding(\n",
    "        input_dim=10000,  # Vocabulary size, will be updated later\n",
    "        output_dim=config['embedding_dim'],\n",
    "        mask_zero=True\n",
    "    )(input_text)\n",
    "    \n",
    "    # BiLSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(\n",
    "        config['lstm_units'],\n",
    "        return_sequences=True\n",
    "    ))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(\n",
    "        config['lstm_units'],\n",
    "        return_sequences=True\n",
    "    ))(x)\n",
    "    \n",
    "    # Output layers\n",
    "    x = layers.TimeDistributed(layers.Dropout(0.2))(x)\n",
    "    outputs = layers.TimeDistributed(\n",
    "        layers.Dense(len(config['labels']), activation='softmax')\n",
    "    )(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=input_text, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def preprocess_text(texts, tokenizer, max_length):\n",
    "    \"\"\"Preprocess text data for transformer models\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentiment analysis data\n",
    "print(\"Preparing sentiment analysis model...\")\n",
    "\n",
    "# Convert sentiment labels to indices\n",
    "label_encoder = LabelEncoder()\n",
    "sentiment_labels = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].values, \n",
    "    sentiment_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['sentiment']['model_name'])\n",
    "\n",
    "# Preprocess data\n",
    "train_data = preprocess_text(train_texts, tokenizer, CONFIG['sentiment']['max_length'])\n",
    "val_data = preprocess_text(val_texts, tokenizer, CONFIG['sentiment']['max_length'])\n",
    "\n",
    "# Build and train model\n",
    "sentiment_model = build_transformer_model('sentiment')\n",
    "\n",
    "history = sentiment_model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    epochs=5,\n",
    "    batch_size=CONFIG['sentiment']['batch_size']\n",
    ")\n",
    "\n",
    "# Save model\n",
    "sentiment_model.save(f'{MODELS_DIR}/sentiment/final')\n",
    "tokenizer.save_pretrained(f'{MODELS_DIR}/sentiment/final')\n",
    "\n",
    "# Test model\n",
    "test_text = \"The road condition is terrible and no one is taking action.\"\n",
    "test_data = preprocess_text(np.array([test_text]), tokenizer, CONFIG['sentiment']['max_length'])\n",
    "prediction = sentiment_model.predict(test_data)\n",
    "predicted_label = CONFIG['sentiment']['labels'][np.argmax(prediction[0])]\n",
    "confidence = np.max(prediction[0])\n",
    "\n",
    "print(f\"\\nTest prediction for: {test_text}\")\n",
    "print(f\"Sentiment: {predicted_label}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification data\n",
    "print(\"Preparing classification model...\")\n",
    "\n",
    "# Convert category labels to indices\n",
    "label_encoder = LabelEncoder()\n",
    "classification_labels = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].values, \n",
    "    classification_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['classification']['model_name'])\n",
    "\n",
    "# Preprocess data\n",
    "train_data = preprocess_text(train_texts, tokenizer, CONFIG['classification']['max_length'])\n",
    "val_data = preprocess_text(val_texts, tokenizer, CONFIG['classification']['max_length'])\n",
    "\n",
    "# Build and train model\n",
    "classifier_model = build_transformer_model('classification')\n",
    "\n",
    "history = classifier_model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    epochs=5,\n",
    "    batch_size=CONFIG['classification']['batch_size']\n",
    ")\n",
    "\n",
    "# Save model\n",
    "classifier_model.save(f'{MODELS_DIR}/classification/final')\n",
    "tokenizer.save_pretrained(f'{MODELS_DIR}/classification/final')\n",
    "\n",
    "# Test model\n",
    "test_text = \"The streetlight in our area has not been working for a week.\"\n",
    "test_data = preprocess_text(np.array([test_text]), tokenizer, CONFIG['classification']['max_length'])\n",
    "prediction = classifier_model.predict(test_data)\n",
    "predicted_label = CONFIG['classification']['labels'][np.argmax(prediction[0])]\n",
    "confidence = np.max(prediction[0])\n",
    "\n",
    "print(f\"\\nTest prediction for: {test_text}\")\n",
    "print(f\"Category: {predicted_label}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da21bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare NER training data\n",
    "print(\"Preparing NER model...\")\n",
    "\n",
    "def prepare_ner_data(df):\n",
    "    \"\"\"Prepare data for NER model\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for text, entities in zip(df['text'], df['entities']):\n",
    "        # Convert text to token sequence\n",
    "        tokens = text.split()  # Simple tokenization, could be improved\n",
    "        \n",
    "        # Create label sequence (one label per token)\n",
    "        label_seq = ['O'] * len(tokens)  # O for Outside\n",
    "        for start, end, label in entities:\n",
    "            # Find token indices for this entity\n",
    "            entity_tokens = text[start:end].split()\n",
    "            for i, token in enumerate(tokens):\n",
    "                if i < len(tokens) - len(entity_tokens) + 1:\n",
    "                    if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                        for j in range(len(entity_tokens)):\n",
    "                            if j == 0:\n",
    "                                label_seq[i+j] = f'B-{label}'  # Beginning\n",
    "                            else:\n",
    "                                label_seq[i+j] = f'I-{label}'  # Inside\n",
    "        \n",
    "        texts.append(tokens)\n",
    "        labels.append(label_seq)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Prepare data\n",
    "texts, labels = prepare_ner_data(df)\n",
    "\n",
    "# Create vocabularies\n",
    "tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Create label mapping\n",
    "unique_labels = set(['O'])\n",
    "for label_seq in labels:\n",
    "    unique_labels.update(label_seq)\n",
    "label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Convert to sequences\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "y = [[label2id[l] for l in seq] for seq in labels]\n",
    "\n",
    "# Pad sequences\n",
    "X_padded = pad_sequences(X, maxlen=CONFIG['ner']['max_length'], padding='post')\n",
    "y_padded = pad_sequences(y, maxlen=CONFIG['ner']['max_length'], padding='post')\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_padded, y_padded,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Update model config with vocabulary size\n",
    "CONFIG['ner']['vocab_size'] = len(tokenizer.word_index) + 1\n",
    "CONFIG['ner']['num_labels'] = len(label2id)\n",
    "\n",
    "# Build and train model\n",
    "ner_model = build_ner_model()\n",
    "\n",
    "history = ner_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=CONFIG['ner']['batch_size']\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "ner_model.save(f'{MODELS_DIR}/ner/final')\n",
    "with open(f'{MODELS_DIR}/ner/tokenizer.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'word_index': tokenizer.word_index,\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label\n",
    "    }, f)\n",
    "\n",
    "# Test model\n",
    "test_text = \"\"\"The drainage system in Gandhi Nagar is completely blocked. \n",
    "Municipal Corporation officer Mr. Kumar has not taken any action despite \n",
    "multiple complaints since January 15th.\"\"\"\n",
    "\n",
    "# Preprocess test text\n",
    "test_tokens = test_text.split()\n",
    "test_seq = tokenizer.texts_to_sequences([test_tokens])\n",
    "test_padded = pad_sequences(test_seq, maxlen=CONFIG['ner']['max_length'], padding='post')\n",
    "\n",
    "# Predict\n",
    "predictions = ner_model.predict(test_padded)\n",
    "predicted_labels = np.argmax(predictions[0], axis=-1)\n",
    "\n",
    "print(\"\\nNER Test Results:\")\n",
    "for token, label_id in zip(test_tokens[:CONFIG['ner']['max_length']], predicted_labels):\n",
    "    if label_id != label2id['O']:  # Only show non-Outside entities\n",
    "        print(f\"{token}: {id2label[label_id]}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ad2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models\n",
    "def test_models():\n",
    "    # Load sentiment model and tokenizer\n",
    "    sentiment_model = keras.models.load_model(f'{MODELS_DIR}/sentiment/final')\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(f'{MODELS_DIR}/sentiment/final')\n",
    "    \n",
    "    # Load classification model and tokenizer\n",
    "    classifier_model = keras.models.load_model(f'{MODELS_DIR}/classification/final')\n",
    "    classifier_tokenizer = AutoTokenizer.from_pretrained(f'{MODELS_DIR}/classification/final')\n",
    "    \n",
    "    # Load NER model and tokenizer\n",
    "    ner_model = keras.models.load_model(f'{MODELS_DIR}/ner/final')\n",
    "    with open(f'{MODELS_DIR}/ner/tokenizer.json', 'r') as f:\n",
    "        ner_data = json.load(f)\n",
    "        word_index = ner_data['word_index']\n",
    "        id2label = ner_data['id2label']\n",
    "    \n",
    "    # Test text\n",
    "    test_text = \"\"\"The drainage system in Gandhi Nagar is completely blocked. \n",
    "    Municipal Corporation officer Mr. Kumar has not taken any action despite \n",
    "    multiple complaints since January 15th.\"\"\"\n",
    "    \n",
    "    # Test sentiment analysis\n",
    "    sentiment_data = preprocess_text(\n",
    "        np.array([test_text]),\n",
    "        sentiment_tokenizer,\n",
    "        CONFIG['sentiment']['max_length']\n",
    "    )\n",
    "    sentiment_pred = sentiment_model.predict(sentiment_data)\n",
    "    sentiment_label = CONFIG['sentiment']['labels'][np.argmax(sentiment_pred[0])]\n",
    "    sentiment_conf = np.max(sentiment_pred[0])\n",
    "    \n",
    "    print(\"Sentiment Analysis:\")\n",
    "    print(f\"Sentiment: {sentiment_label}\")\n",
    "    print(f\"Confidence: {sentiment_conf:.2f}\\n\")\n",
    "    \n",
    "    # Test classification\n",
    "    class_data = preprocess_text(\n",
    "        np.array([test_text]),\n",
    "        classifier_tokenizer,\n",
    "        CONFIG['classification']['max_length']\n",
    "    )\n",
    "    class_pred = classifier_model.predict(class_data)\n",
    "    class_label = CONFIG['classification']['labels'][np.argmax(class_pred[0])]\n",
    "    class_conf = np.max(class_pred[0])\n",
    "    \n",
    "    print(\"Category Classification:\")\n",
    "    print(f\"Category: {class_label}\")\n",
    "    print(f\"Confidence: {class_conf:.2f}\\n\")\n",
    "    \n",
    "    # Test NER\n",
    "    tokens = test_text.split()\n",
    "    test_seq = [[word_index.get(word.lower(), word_index['<UNK>']) for word in tokens]]\n",
    "    test_padded = pad_sequences(test_seq, maxlen=CONFIG['ner']['max_length'], padding='post')\n",
    "    \n",
    "    ner_pred = ner_model.predict(test_padded)\n",
    "    pred_labels = np.argmax(ner_pred[0], axis=-1)\n",
    "    \n",
    "    print(\"Named Entity Recognition:\")\n",
    "    current_entity = None\n",
    "    current_text = []\n",
    "    \n",
    "    for token, label_id in zip(tokens[:CONFIG['ner']['max_length']], pred_labels):\n",
    "        label = id2label[str(label_id)]\n",
    "        if label != 'O':\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    print(f\"{''.join(current_text)}: {current_entity}\")\n",
    "                current_entity = label[2:]\n",
    "                current_text = [token]\n",
    "            elif label.startswith('I-'):\n",
    "                if current_entity == label[2:]:\n",
    "                    current_text.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    print(f\"{''.join(current_text)}: {current_entity}\")\n",
    "                current_entity = label\n",
    "                current_text = [token]\n",
    "    \n",
    "    if current_entity:\n",
    "        print(f\"{''.join(current_text)}: {current_entity}\")\n",
    "\n",
    "# Run tests\n",
    "print(\"Testing all models...\\n\")\n",
    "test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8639c",
   "metadata": {},
   "source": [
    "# Model Integration Instructions\n",
    "\n",
    "After training, the models will be saved in the `saved_models` directory:\n",
    "- Sentiment Analysis: `saved_models/sentiment/final`\n",
    "- Text Classification: `saved_models/classification/final`\n",
    "- NER: `saved_models/ner/model-best`\n",
    "\n",
    "To use these models in the Django application:\n",
    "\n",
    "1. Update the model paths in `views.py`:\n",
    "```python\n",
    "sentiment_analyzer = pipeline('text-classification', model='./saved_models/sentiment/final')\n",
    "text_classifier = pipeline('text-classification', model='./saved_models/classification/final')\n",
    "nlp = spacy.load('./saved_models/ner/model-best')\n",
    "```\n",
    "\n",
    "2. The models can now handle:\n",
    "   - Sentiment analysis in multiple languages\n",
    "   - Classification of complaints into categories\n",
    "   - Named entity recognition for important information\n",
    "\n",
    "3. Each model provides confidence scores that can be used to:\n",
    "   - Filter low-confidence predictions\n",
    "   - Route uncertain cases to human reviewers\n",
    "   - Monitor model performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
